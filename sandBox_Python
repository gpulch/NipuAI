# 3 fonctions pour craper le site leCigare
La première fonction scrapingPages() passe dans les pages de listing des produits et récupère les url de tous les produits listés
La seconde scrapingProduit() récupère dans la page produit les informations nécessaires
La troisième scrapingLoop() fait une itération sur tous les produits et constitue un DataFrame (csv)



import time
from bs4 import BeautifulSoup
import pandas as pd
import requests
import re
import random



def scrapingPages(url, premièrePage, dernièrePage, requêtageBorneMin = 5, requêtageBorneMax = 10):

    #Initialisation des variables
    numéroPage = premièrePage
    liens = []
    
    while numéroPage <= dernièrePage:
        url_page = url +str(numéroPage)
        
        # Envoyer une requête GET
        response = requests.get(url_page)
        
        # Vérifier le code de réponse
        if response.status_code == 200:
            
            # Le code de réponse est 200 (OK), vous pouvez procéder au scraping
            codeHtml_page = response.text

            #Navigation dans les balises HTML
            soup_page = BeautifulSoup(codeHtml_page, 'html.parser')

            # Trouver toutes les balises <a> avec la classe "thumbnail product-thumbnail"
            balises_a = soup_page.find_all('a', class_='thumbnail product-thumbnail')

            # Parcourir toutes les balises <a> et extraire les valeurs de l'attribut "href"
            for balise_a in balises_a:
                lien = balise_a.get('href')
                liens.append(lien)

            # Master liste
            liens.append(lien)

            # Incrémentation boucle while
            numéroPage += 1

            # Requêtage différé
            time.sleep(random.uniform(requêtageBorneMin, requêtageBorneMax))
            
        else:
            # Le code de réponse n'est pas 200, il peut y avoir une erreur.
            print("La requête a retourné un code de réponse non valide :", response.status_code)
        
    # Retour fonction
    return liens


def scrapingProduit(url_produit, siteInternet):

    # Envoyer une requête GET
    response = requests.get(url_produit)
        
    # Vérifier le code de réponse
    if response.status_code == 200:
            
        # Le code de réponse est 200 (OK), vous pouvez procéder au scraping
        codeHtml_produit = response.text

        #Navigation dans les balises HTML
        soup_produit = BeautifulSoup(codeHtml_produit, 'html.parser')

        # Balises <hl>
        balise_h1 = soup_produit.find('h1')

        # Extraire le texte de la balise <h1>
        texte_balise_h1 = balise_h1.text

        # Trouver toutes les balises <span> avec l'attribut "itemprop" égal à "price"
        balises_span = soup_produit.find_all('span', itemprop='price')

        # Parcourir toutes les balises <span> et extraire les informations
        for balise_span in balises_span:
            # Extraire le texte de la balise <span>
            texte = balise_span.text.strip()

            # Utiliser une expression régulière pour extraire le prix (en nombre) et la devise (en lettres)
            correspondance = re.match(r'([\d,.]+)\s*([\w]+)', texte)

            if correspondance:
                # Récupérer le prix (en nombre) et la devise (en lettres)
                prix = []
                devise = []
                prix.append(correspondance.group(1))
                devise.append(correspondance.group(2))

        # Trouver toutes les balises <p> qui contiennent du texte fort <strong>
        balises_p = soup_produit.find_all('p')
        texte_strong = []

        for balise_p in balises_p:
            texte_fort = balise_p.find('strong')
            if texte_fort:
                texte_strong.append(texte_fort.text.strip())

        quantité = texte_strong[2]

        # Balises <dl>
        balise_dl = soup_produit.find("dl", class_="data-sheet")

        # Extraire le texte de la balise <dl> incluant ses enfants <dt> et <dd>
        texte_balise_dl = balise_dl.text

        # Diviser le texte en paires de titres et de valeurs
        paires = texte_balise_dl.split('\n')
        # Supprimer les chaînes vides de la liste
        paires = [element.strip() for element in paires if element.strip()]


        # Créer un dictionnaire 'data frame' à partir des paires et ajouter le nom, le prix et la devise
        df = []
        for i in range(1, len(paires), 2):
            df.append(paires[i])
        df = df + [texte_balise_h1, prix[0], devise[0], quantité, siteInternet]
    
    else:
        # Le code de réponse n'est pas 200, il peut y avoir une erreur.
        print("La requête a retourné un code de réponse non valide :", response.status_code)
    
    #Retour de la fonction
    return df


def scrapingloop(listeURL, siteInternet = 'LeCigare', requêtageBorneMin = 5, requêtageBorneMax = 10):
    
    noms_de_colonnes = ['Puissance', 'Ring', 'Durée', 'Arômes', 'Diamètre', 'Longueur', 'Module', 'Pays', 'Nom', 'Prix', 'Devise', 'Quantité', 'Site']
    master_df = pd.DataFrame(columns=noms_de_colonnes)

    for element in listeURL:
        try:
            df_provisoire = pd.DataFrame([scrapingProduit(element, siteInternet)], columns=noms_de_colonnes)
            master_df = pd.concat([master_df, df_provisoire], axis=0, ignore_index=True)
        except:
            df_provisoire = pd.DataFrame([], columns=noms_de_colonnes)
            master_df = pd.concat([master_df, df_provisoire], axis=0, ignore_index=True)

        # Requêtage différé
        time.sleep(random.uniform(requêtageBorneMin, requêtageBorneMax))

    return master_df

